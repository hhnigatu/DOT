{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Duplicate Page Detection \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Process the dataset and generate the required dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required fuctions from the library.\n",
    "from exact_duplicate_detection_functions import HashPages, threshold_by_percent, threshold_by_number_of_matched_pages\n",
    "# run HashPages Function to get the duplicate info on file level, a datafrmae for visualization and a dataframe with representative pages selected for processing.\n",
    "file_level_df, reduced_df=HashPages(dataset_path=\"PATH\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 2: Provide Conditions for Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set conditions for filtering exact duplciates\n",
    "condition_1=threshold_by_percent(file_level_df, min_page_in_file=2, max_page_in_file=20,min_percentage_value=40)\n",
    "condition_2=threshold_by_number_of_matched_pages(file_level_df, max_page_in_file=10, max_page_in_file_match=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Visualize the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import functions for visualizing\n",
    "from exact_duplicate_detection_functions import visualize_file_pairs, print_duplicate_info\n",
    "\n",
    "print_duplicate_info([condition_2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_1='PATH'\n",
    "path_2='PATH'\n",
    "file_a, file_b=visualize_file_pairs(path_1, path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Near Duplicate Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Classify the Pages in PDFs based on their type and calculate correlation between pairs of pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the classifer and pair correlation function and pass the dataset path along with the name of\n",
    "# the page types we want to find near duplciates for. \n",
    "from near_duplicate_detection_functions import ClassifyAndGetPairCorrelation\n",
    "correlation_df=ClassifyAndGetPairCorrelation('PATH', ['form', 'image', 'narrative'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Visualize Correlation Value in Pairs and Identify Threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from near_duplicate_detection_functions import plot_pairs_of_pages\n",
    "plot_pairs_of_pages(correlation_df, ['form'], 10,  threshold=0.6, random=True, sort_acending=False)\n",
    "plot_pairs_of_pages(correlation_df, ['image'], 3 ,threshold=0, random=True, sort_acending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set correlation threshold and save filtered dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from near_duplicate_detection_functions import set_correlation_threshold, print_near_duplciate_information\n",
    "filtered_df=set_correlation_threshold(correlation_df, {'form': 0.9, 'image': 0.77})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_near_duplciate_information(filtered_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize Pairs of files with near duplicate pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from near_duplicate_detection_functions import visualize_file_pairs\n",
    "sample_8, sample_14=visualize_file_pairs('sample 8.pdf','sample 14.pdf')\n",
    "sample_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
